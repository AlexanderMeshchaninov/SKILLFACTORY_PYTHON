{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ОБУЧЕНИЕ С УЧИТЕЛЕМ - ЛОГИСТИЧЕСКАЯ РЕГРЕССИЯ\n",
    "\n",
    "<img src='Images/ml_1.png'>\n",
    "\n",
    "Задача классификации (classification) — задача, в которой мы пытаемся предсказать класс объекта на основе признаков в наборе данных. То есть задача сводится к предсказанию целевого признака, который является категориальным.\n",
    "\n",
    "Когда классов, которые мы хотим предсказать, только два, классификация называется **бинарной**. Например, мы можем предсказать, болен ли пациент раком, является ли изображение человеческим лицом, является ли письмо спамом и т. д.\n",
    "\n",
    "Когда классов, которые мы хотим предсказать, более двух, классификация называется **мультиклассовой (многоклассовой)**. Например, предсказание модели самолёта по радиолокационным снимкам, классификация животных на фотографиях, определение языка, на котором говорит пользователь, разделение писем на группы.\n",
    "\n",
    "Что вообще означает «решить задачу классификации»? Это значит построить разделяющую поверхность в пространстве признаков, которая делит пространство на части, каждая из которых соответствует определённому классу. \n",
    "\n",
    "Ниже представлены примеры разделяющих поверхностей, которые производят бинарную классификацию. Красным и синим цветом обозначены классы, зелёным — собственно поверхность, которая делит пространство признаков на две части. В каждой из этих частей находятся только наблюдения определённого класса.\n",
    "\n",
    "<img src='Images/ml_2.png' height='300' width='1000'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели, которые решают задачу классификации, называются **классификаторами (classifier)**.\n",
    "\n",
    "# ОБЩЕЕ ПРЕДСТАВЛЕНИЕ О ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ\n",
    "\n",
    "**Логистическая регрессия (Logistic Regression)** — одна из простейших моделей для решения задачи классификации. Несмотря на простоту, модель входит в топ часто используемых алгоритмов классификации в Data Science.\n",
    "\n",
    "В основе логистической регрессии лежит логистическая функция (*logistic function*) $ \\sigma (z) $ — отсюда и название модели. Однако более распространённое название этой функции — **сигмоида (sigmoid)**. Записывается она следующим образом:\n",
    "\n",
    "$$ \\sigma (z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "<img src='Images/ml_3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У сигмоиды есть два очень важных для нас свойства:\n",
    "\n",
    "- Значения сигмоиды $ \\sigma (z) $ лежат в диапазоне от 0 до 1 при любых значениях аргумента $z$: какой бы $z$ вы ни подставили, число меньше 0 или больше 1 вы не получите.\n",
    "\n",
    "- Сигмоида выдаёт значения $ \\sigma (z) > 0.5 $ при её аргументе $z > 0$, $ \\sigma (z) < 0.5 $ — при $z < 0$ и $ \\sigma (z) = 0.5 $ — при $z = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основой алгоритма является **логистическая функция** (сигмоида), которая преобразует линейную комбинацию признаков в значение вероятности от 0 до 1.\n",
    "\n",
    "**Логистическая функция (сигмоида):**  \n",
    "$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $\n",
    "\n",
    "где $z = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_n x_n$ — линейная комбинация признаков.\n",
    "\n",
    "**Решающее правило:**  \n",
    "$ \n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "1, & \\text{если } \\sigma(z) \\geq 0.5 \\\\\n",
    "0, & \\text{если } \\sigma(z) < 0.5\n",
    "\\end{cases} \n",
    "$\n",
    "\n",
    "**Функция потерь для логистической регрессии** — это **логистическая (кросс-энтропийная) ошибка**:  \n",
    "$ \n",
    "L(y, \\hat{y}) = - \\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] \n",
    "$\n",
    "\n",
    "Логистическая регрессия предсказывает вероятность класса и выбирает класс с наибольшей вероятностью (например, $y = 1$ или $y = 0$).\n",
    "\n",
    "<center> <img src='Images/ml_4.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим, как это работает, на примере.\n",
    "\n",
    "Пусть мы каким-то образом обучили модель линейной регрессии предсказывать положительные числа для спам-писем и отрицательные — для обычных писем.\n",
    "\n",
    "Подаём характеристики письма $x_1, x_2, \\dots, x_m$ в выражение для линейной регрессии и получаем ответ модели, например $z = 1.5$. Тогда, подставив его в сигмоиду, получим:\n",
    "\n",
    "$$\n",
    "\\hat{P} = \\sigma(z) = \\frac{1}{1 + e^{-z}} = \\frac{1}{1 + e^{-1.5}} = 0.82\n",
    "$$\n",
    "\n",
    "Таким образом, вероятность того, что данный объект принадлежит классу спама, равна 0.82, что больше порогового значения 0.5. То есть мы относим данное письмо к спаму: $\\hat{y} = 1$.\n",
    "\n",
    "Пусть теперь мы подали на вход модели характеристики другого письма и получили $z = -0.91$. Тогда, подставив этот результат в сигмоиду, получим:\n",
    "\n",
    "$$\n",
    "\\hat{P} = \\sigma(z) = \\frac{1}{1 + e^{-z}} = \\frac{1}{1 + e^{0.91}} = 0.28\n",
    "$$\n",
    "\n",
    "Вероятность того, что данный объект принадлежит классу спама, равна 0.28, что меньше порогового значения 0.5. Мы относим данное письмо к обычным письмам: $\\hat{y} = 0$.\n",
    "\n",
    "Кстати, вероятность того, что это письмо будет обычным, равна противоположной вероятности: $Q = 1 - 0.28 = 0.72$.\n",
    "\n",
    "Полученное выражение для оценки вероятности $\\hat{P}$ и будет называться моделью логистической регрессии:\n",
    "\n",
    "$$\n",
    "\\hat{P} = \\frac{1}{1 + e^{- \\left( w_0 + \\sum_{j=1}^{m} w_j x_j \\right) }}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y} = I \\left[ \\hat{P} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример\n",
    "\n",
    "Мы пытаемся предсказать поступление студента в аспирантуру в зависимости от результатов двух экзаменов. Целевой признак $y$ — результат поступления (*admission outcome*) с двумя возможными значениями: поступил или не поступил. Факторы: $x_1$ — результат сдачи первого экзамена (*Exam 1 Score*) и $x_2$ — результат сдачи второго экзамена (*Exam 2 Score*). Будем предсказывать вероятность поступления с помощью логистической регрессии.\n",
    "\n",
    "Изобразим зависимость в пространстве двух факторов (вид сверху) в виде диаграммы рассеяния, а целевой признак отобразим в виде точек (непоступившие) и крестиков (поступившие).\n",
    "\n",
    "Если рассматривать уравнение линейной регрессии отдельно от сигмоиды, то геометрически построить логистическую регрессию на основе двух факторов — значит найти такие коэффициенты $w_0, w_1$ и $w_2$ уравнения плоскости, при которых наблюдается наилучшее разделение пространства на две части.\n",
    "\n",
    "$$ z = w_0 + w_1 x_1 + w_2 x_2 $$\n",
    "\n",
    "Тогда выражение для $z$ будет задавать в таком пространстве плоскость (в проекции вида сверху — прямую), которая разделяет всё пространство на две части. Над прямой вероятность поступления будет $> 0.5$, а под прямой — $< 0.5$.\n",
    "\n",
    "<img src='Images/ml_5.png' height='400' width='600'>\n",
    "\n",
    "Коэффициенты построенной выше плоскости равны (как их найти, обсудим позже):\n",
    "\n",
    "$w_0 = -25.05$  \n",
    "$w_1 = 0.205$  \n",
    "$w_2 = 0.2$  \n",
    "\n",
    "Тогда модель логистической регрессии будет иметь вид:\n",
    "\n",
    "$z = -25.05 + 0.205 x_1 + 0.2 x_2$\n",
    "\n",
    "$\\hat{P} = \\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "Появляется новый абитуриент, и мы хотим предсказать вероятность его поступления. Баллы студента: $x_1 = 67, x_2 = 53$.  \n",
    "Заметьте, что точка с такими координатами находится ниже нашей плоскости (то есть абитуриент, скорее всего, не поступит).\n",
    "\n",
    "Тогда:\n",
    "\n",
    "$z = -25.05 + 0.205 \\cdot 67 + 0.2 \\cdot 53 = -0.71$\n",
    "\n",
    "$\\hat{P} = \\sigma(z) = \\frac{1}{1 + e^{0.71}} = 0.32$\n",
    "\n",
    "Итак, оценка вероятности поступления студента составляет 0.32, то есть его шанс поступления составляет 32%.\n",
    "\n",
    "А что если мы возьмём точку, лежащую выше прямой?\n",
    "\n",
    "Например, появился абитуриент с баллами $x_1 = 80, x_2 = 75$. Подставим его баллы в нашу модель логистической регрессии, чтобы понять, какова оценочная вероятность поступления:\n",
    "\n",
    "$z = -25.05 + 0.205 \\cdot 80 + 0.2 \\cdot 75 = 6.34$\n",
    "\n",
    "$\\hat{P} = \\sigma(z) = \\frac{1}{1 + e^{-6.34}} = 0.99$\n",
    "\n",
    "Таким образом, оценка вероятности поступления абитуриента составляет 0.99, шанс поступления — 99%.\n",
    "\n",
    "В общем случае, когда у нас есть зависимость от $m$ факторов, линейное выражение, находящееся под сигмоидой, будет обозначать **разделяющую гиперплоскость**:\n",
    "\n",
    "$$ z = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_m x_m = w_0 + \\sum_{j=1}^{m} w_j x_j $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ЛОГИСТИЧЕСКОЕ ПРЕОБРАЗОВАНИЕ\n",
    "\n",
    "Логистическое преобразование — это применение сигмоидной функции для преобразования линейной комбинации признаков в значение в диапазоне от 0 до 1. Оно используется в логистической регрессии для моделирования вероятности принадлежности к определённому классу:\n",
    "\n",
    "$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $\n",
    "​\n",
    "\n",
    "Где $z = w_0 + \\sum_{j=1}^{m} w_j x_j$.\n",
    "\n",
    "Результат логистического преобразования интерпретируется как вероятность, на основе которой принимается решение о принадлежности к классу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ПОИСК ПАРАМЕТРОВ ЛОГИСТИЧЕСКОЙ РЕГРЕССИИ\n",
    "\n",
    "Итак, мы разобрались с тем, как выглядит модель логистической регрессии и что она означает в геометрическом смысле.\n",
    "\n",
    "Но остался один главный вопрос: как найти такие коэффициенты $w = (w_0, w_1, w_2, \\dots, w_m)$, чтобы гиперплоскость разделяла пространство наилучшим образом?\n",
    "\n",
    "Здесь нужен другой подход (не такой какой мы использовали в линейной регрессии). Это метод максимального правдоподобия (Maximum Likelihood Estimation — MLE). \n",
    "\n",
    "Правдоподобие — это оценка того, насколько вероятно получить истинное значение целевой переменной $y$ при данных $x$ и параметрах $w$. \n",
    "\n",
    "Данный метод позволяет получить функцию правдоподобия.\n",
    "\n",
    "## Функция правдоподобия\n",
    "\n",
    "$likelihood = \\sum_{i=1}^{n} \\left( y_i \\log(\\hat{P_i}) + (1 - y_i) \\log(1 - \\hat{P_i}) \\right) \\to \\max_w$\n",
    "\n",
    "Где:\n",
    "- $n$ — количество наблюдений.\n",
    "- $y_i$ — это истинный класс (1 или 0) для $i$-го объекта из набора данных.\n",
    "- $\\hat{P_i} = \\sigma(z_i)$ — предсказанная с помощью логистической регрессии вероятность принадлежности к классу 1 для $i$-го объекта из набора данных.\n",
    "- $z_i$ — результат подстановки $i$-го объекта из набора данных в уравнение разделяющей плоскости $z_i = \\hat{w} \\cdot \\bar{x_i}$.\n",
    "- $\\log$ — логарифм (обычно используется натуральный логарифм по основанию $e$ — $ln$).\n",
    "\n",
    "---\n",
    "\n",
    "### Пример расчёта функции правдоподобия\n",
    "\n",
    "Вернёмся к примеру с абитуриентами. Пусть у нас есть выборка из четырёх студентов с оценками по двум экзаменам: $x_1$ и $x_2$. Возьмём уравнение разделяющей плоскости, которое мы использовали ранее:\n",
    "\n",
    "$z = -25.05 + 0.205x_1 + 0.2x_2$\n",
    "\n",
    "Мы взяли всех студентов из выборки и подставили их в формулу сигмоиды, получив оценочную вероятность поступления каждого из студентов:\n",
    "\n",
    "| $i$ | $\\hat{P_i}$ — оценка вероятности | $y_i$ — истинный класс |\n",
    "|-----|----------------------------------|-------------------------|\n",
    "| 1   | 0.2                              | 0                       |\n",
    "| 2   | 0.8                              | 0                       |\n",
    "| 3   | 1                                | 1                       |\n",
    "| 4   | 0.6                              | 1                       |\n",
    "\n",
    "Подсчитаем, чему равна функция правдоподобия при данных предсказаниях вероятностей:\n",
    "\n",
    "$likelihood = \\sum_{i=1}^{n} \\left( y_i \\log(\\hat{P_i}) + (1 - y_i) \\log(1 - \\hat{P_i}) \\right)$\n",
    "\n",
    "$= \\left( 0 \\log(0.2) + (1 - 0) \\log(1 - 0.2) \\right) + \\left( 0 \\log(0.8) + (1 - 0) \\log(1 - 0.8) \\right)$\n",
    "\n",
    "$+ \\left( 1 \\log(1) + (1 - 1) \\log(1 - 1) \\right) + \\left( 1 \\log(0.6) + (1 - 1) \\log(1 - 0.6) \\right)$\n",
    "\n",
    "$= \\left( \\log(0.8) \\right) + \\left( \\log(0.2) \\right) + \\left( \\log(1) \\right) + \\left( \\log(0.6) \\right)$\n",
    "\n",
    "$= \\log(0.8) + \\log(0.2) + \\log(1) + \\log(0.6) = -2.34$\n",
    "\n",
    "Примечание: К сожалению, функция likelihood не имеет интерпретации, то есть нельзя сказать, что значит число -2.34 в контексте правдоподобия.\n",
    "\n",
    "Цель — найти такие параметры, при которых наблюдается максимум этой функции.\n",
    "\n",
    "Теперь пора снова применить магию математики, чтобы привести задачу к привычному нам формату минимизации эмпирического риска. По правилам оптимизации, если поставить перед функцией минус, то задача оптимизации меняется на противоположную: был поиск максимума — станет поиском минимума.\n",
    "\n",
    "Таким образом мы получим функцию потерь $L(w)$, которая носит название «функция логистических потерь», или logloss. Также часто можно встретить название кросс-энтропия, или cross-entropy loss.\n",
    "\n",
    "## Градиентный спуск для функции логистической потери (Log Loss)\n",
    "\n",
    "Формула обновления весов $w_j$ с использованием градиентного спуска для функции логистической потери:\n",
    "\n",
    "$$\n",
    "w_j := w_j - \\alpha \\frac{\\partial Loss}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "Где:\n",
    "- $w_j$ — текущий вес для $j$-го признака.\n",
    "- $\\alpha$ — темп обучения (learning rate).\n",
    "- $\\frac{\\partial Loss}{\\partial w_j}$ — частная производная функции логистической потери по весу $w_j$.\n",
    "\n",
    "### Производная функции логистической потери:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Loss}{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\hat{P_i} - y_i \\right) x_{ij}\n",
    "$$\n",
    "\n",
    "Где:\n",
    "- $n$ — количество наблюдений.\n",
    "- $\\hat{P_i} = \\sigma(z_i) = \\frac{1}{1 + e^{-z_i}}$ — предсказанная вероятность для $i$-го объекта.\n",
    "- $y_i$ — истинный класс для $i$-го объекта (0 или 1).\n",
    "- $x_{ij}$ — значение $j$-го признака для $i$-го объекта.\n",
    "\n",
    "Таким образом, веса $w_j$ обновляются в направлении, противоположном градиенту, чтобы минимизировать функцию логистической потери.\n",
    "\n",
    "Формула градиентного спуска для логистической регрессии используется для оптимизации весов модели, то есть для нахождения таких значений весов, которые минимизируют функцию логистической потери (log-loss).\n",
    "\n",
    "Основные цели использования:\n",
    "\n",
    "1. Минимизация ошибки:\n",
    "\n",
    "* Функция log-loss измеряет, насколько хорошо модель предсказывает вероятности классов. Цель — уменьшить это значение, чтобы повысить точность предсказаний.\n",
    "\n",
    "2. Обучение модели:\n",
    "\n",
    "* В процессе обучения логистической регрессии, веса модели (коэффициенты) последовательно обновляются с помощью градиентного спуска, чтобы приблизиться к оптимальным значениям.\n",
    "\n",
    "3. Итеративное улучшение:\n",
    "\n",
    "* Градиентный спуск — это итеративный алгоритм, который на каждом шаге обновляет веса так, чтобы уменьшить значение log-loss. С каждой итерацией модель становится точнее.\n",
    "\n",
    "4. Нахождение наилучшей разделяющей гиперплоскости:\n",
    "\n",
    "* В контексте задач классификации (например, бинарной), формула помогает найти такую комбинацию весов, которая лучше всего разделяет данные на два класса."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрики классификации. Преимущества и недостатки логистической регрессии\n",
    "\n",
    "### Ошибка I и II рода\n",
    "\n",
    "Пусть у нас есть некоторый пациент $x_i$, и мы хотим понять, болен ли он диабетом. С точки зрения задачи классификации мы хотим предсказать истинный класс ($y_i$) пациента.\n",
    "\n",
    "Нулевая гипотеза будет состоять в отсутствии эффекта (пациент не болен диабетом), то есть $y_i = 0$, а альтернативная — в его наличии (пациент болен диабетом), то есть $y_i = 1$. В терминах статистических гипотез это будет записано так:\n",
    "\n",
    "- $H_0$: Пациент $x_i$ не болеет диабетом $(y_i = 0)$.\n",
    "- $H_1$: Пациент $x_i$ болеет диабетом $(y_i = 1)$.\n",
    "\n",
    "Тогда у нас есть два случая, в которых мы можем допустить ошибку:\n",
    "\n",
    "- **Ошибка I (первого) рода** ($\\alpha$-ошибка): отклонение нулевой гипотезы, когда она на самом деле верна, или **ложноположительный результат**.  \n",
    "  То есть мы предсказали, что пациент болен диабетом, хотя это не так.\n",
    "\n",
    "- **Ошибка II (второго) рода** ($\\beta$-ошибка): принятие нулевой гипотезы, когда она на самом деле ложна, или **ложноотрицательный результат**.  \n",
    "  То есть мы предсказали, что пациент здоров, хотя на самом деле он болен диабетом.\n",
    "\n",
    "---\n",
    "\n",
    "**Примечание:** Как вы можете понять, в диагностических задачах для нас критичнее ошибка II рода. Последствия будут более серьёзными, если мы примем больного пациента за здорового, чем если мы примем здорового за больного. Нам важно охватить всех потенциально больных пациентов, чтобы сделать дополнительный анализ и удостовериться в результате."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все метрики, которые мы рассмотрим, основаны на матрице ошибок. С неё мы и начнём наш разбор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. МАТРИЦА ОШИБОК\n",
    "\n",
    "Матрица ошибок — это таблица, которая показывает результаты работы модели классификации, сравнивая предсказанные и реальные классы. Она состоит из 4 элементов:\n",
    "\n",
    "- **TP (True Positive)** — Верно предсказанный положительный класс (например, модель правильно определила больного пациента).\n",
    "- **TN (True Negative)** — Верно предсказанный отрицательный класс (модель правильно определила здорового пациента).\n",
    "- **FP (False Positive)** — Ложноположительный результат (здоровый пациент предсказан как больной).\n",
    "- **FN (False Negative)** — Ложноотрицательный результат (больной пациент предсказан как здоровый).\n",
    "\n",
    "### Пример\n",
    "Рассмотрим задачу предсказания диабета (1 — болен, 0 — здоров):\n",
    "\n",
    "|               | **Предсказано 0** | **Предсказано 1** |\n",
    "|---------------|------------------|------------------|\n",
    "| **Реальное 0** | TN (50)          | FP (5)           |\n",
    "| **Реальное 1** | FN (3)           | TP (42)          |\n",
    "\n",
    "- **TP = 42**: 42 больных правильно классифицированы.\n",
    "- **TN = 50**: 50 здоровых правильно классифицированы.\n",
    "- **FP = 5**: 5 здоровых ошибочно предсказаны как больные.\n",
    "- **FN = 3**: 3 больных ошибочно предсказаны как здоровые.\n",
    "\n",
    "Матрица ошибок помогает оценить качество модели и вычислить метрики, такие как точность (accuracy), полнота (recall) и точность классификации (precision).\n",
    "\n",
    "Давайте посмотрим, как будет выглядеть матрица ошибок для нашего примера предсказаний модели `log_reg_full`:\n",
    "\n",
    "|              | $\\hat{y} = 0$ | $\\hat{y} = 1$ |\n",
    "|--------------|---------------|---------------|\n",
    "| $y = 0$      | TN = 1        | FP = 2        |\n",
    "| $y = 1$      | FN = 3        | TP = 4        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='Images/ml_8.png' height='600' width='700'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Accuracy (достоверность/аккуратность)\n",
    "\n",
    "**Accuracy** — это доля правильных ответов модели среди всех ответов. Правильные ответы включают:\n",
    "\n",
    "- **TP** (True Positive) — истинно положительные.\n",
    "- **TN** (True Negative) — истинно отрицательные.\n",
    "\n",
    "Формула для вычисления:\n",
    "\n",
    "$$\n",
    "accuracy = \\frac{TP + TN}{TP + TN + FN + FP}\n",
    "$$\n",
    "\n",
    "**Примечание:** В русской литературе метрику **accuracy** иногда переводят как «точность», однако «precision» также переводится как «точность». Поэтому старайтесь указывать, о какой именно метрике идёт речь — **accuracy** или **precision**.\n",
    "\n",
    "---\n",
    "\n",
    "**Диаграмма:**  \n",
    "Соотношение количества объектов, классы которых угадала модель, и общего количества объектов можно записать так:\n",
    "\n",
    "<center><img src='Images/ml_6.png' height='400' width='350'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интерпретация: \n",
    "Как много (в долях) модель угадала ответов.\n",
    "\n",
    "Метка **accuracy** изменяется в диапазоне от 0 до 1. Чем ближе значение к 1, тем больше ответов модель «угадала».\n",
    "\n",
    "#### Рассчитаем accuracy для нашего примера:\n",
    "$$\n",
    "accuracy = \\frac{4 + 1}{4 + 2 + 3 + 1} = 0.5\n",
    "$$\n",
    "\n",
    "Итак, наша **accuracy** равна 0.5, то есть модель сделала верное предсказание для 50% пациентов из выборки.\n",
    "\n",
    "---\n",
    "\n",
    "**Примечание:**\n",
    "\n",
    "Accuracy — самая простая и самая понятная метрика классификации, но у неё есть один существенный недостаток. Она бесполезна, если классы сильно несбалансированы. Продемонстрируем это следующим примером."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Precision (точность), или PPV (Positive Predictive Value)\n",
    "Это доля объектов, названных классификатором положительными и при этом действительно являющихся таковыми, по отношению ко всем названным положительными объектам.\n",
    "\n",
    "$$\n",
    "precision = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "В виде диаграммы соотношение количества объектов класса 1, которые мы угадали, и количества объектов, которые мы приняли за класс 1, записывается следующим образом:\n",
    "\n",
    "<center><img src='Images/ml_7.png' height='300' width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрика также изменяется от 0 до 1. \n",
    "\n",
    "Интерпретация: способность отделить класс 1 от класса 0. Чем больше precision, тем меньше ложных попаданий. То есть чем ближе precision к 1, тем меньше вероятность модели допустить ошибку I рода.\n",
    "\n",
    "Именно precision не позволяет записать все ответы в один класс, так как в таком случае резко возрастает значение False Positive и метрика снижается.\n",
    "\n",
    "Рассчитаем precision для нашего примера:\n",
    "\n",
    "$$\n",
    "precision = \\frac{4}{4 + 2} = 0.67\n",
    "$$\n",
    "\n",
    "Таким образом, количество названных моделью больных диабетом и при этом действительно являющихся больными составляет 67 % от всех пациентов.\n",
    "\n",
    "Precision нужен в задачах, где от нас требуется минимум ложных срабатываний. Чем выше «цена» ложноположительного результата, тем выше должен быть precision.\n",
    "\n",
    "Например, по камерам видеонаблюдения мы автоматически выявляем признаки драки на улицах и отправляем наряд полиции для урегулирования конфликта. Однако штат сотрудников сильно ограничен, реагировать на любой признак конфликта мы не можем, поэтому мы хотим уменьшить количество ложных вызовов. В такой ситуации мы выберем такую модель, у которой наибольший precision.\n",
    "\n",
    "В предельном случае (когда precision равен 1) у модели отсутствуют ложноположительные срабатывания.\n",
    "\n",
    "Примечание: Важно понимать, что данный вывод справедлив только для выборки, на которой мы оцениваем метрику, то есть это не означает, что модель вовсе не может допустить ложноположительных результатов. Однако чем больше выборка, на которой мы тестируем алгоритм, тем ближе к истине будет данный вывод."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Recall (полнота), или TPR (True Positive Rate)\n",
    "\n",
    "Это доля объектов, названных классификатором положительными и при этом действительно являющихся таковыми, по отношению ко всем объектам положительного класса.\n",
    "\n",
    "$$\n",
    "recall = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "<center><img src='Images/ml_9.png' height='350' width='300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интерпретация: способность модели обнаруживать класс 1 вообще, то есть охват класса 1. Заметьте, что метрика зависит от количества ложноотрицательных срабатываний. То есть чем ближе recall к 1, тем меньше вероятность модели допустить ошибку II рода.\n",
    "\n",
    "Рассчитаем recall для нашего примера:\n",
    "\n",
    "$$\n",
    "recall = \\frac{4}{4 + 3} = 0.57\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, процент пациентов, которых модель определила к классу больных диабетом, среди всех действительно больных диабетом составляет 57 %.\n",
    "\n",
    "Recall очень хорошо себя показывает в задачах, где важно найти как можно больше объектов, принадлежащих к классу 1.\n",
    "\n",
    "Например, это различные задачи, в которых мы пытаемся определить наличие эффекта, который может привести к серьёзным последствиям. Это могут быть те же диагностические задачи или задачи, в которых мы прогнозируем вероятность отказа устройства, от работы которого зависят человеческие жизни.\n",
    "\n",
    "Предельный случай (когда recall равен 1) означает, что модель нашла все объекты класса 1, например всех действительно больных пациентов. Однако метрика ничего не скажет о том, с какой точностью мы это сделали.\n",
    "\n",
    "Примечание: Важно понимать, что данный вывод справедлив только для выборки, на которой мы оцениваем метрику, то есть это не означает, что модель вовсе не может допустить ложноотрицательных исходов. Однако чем больше выборка, на которой мы тестируем алгоритм, тем данный вывод будет ближе к истине.\n",
    "\n",
    "Метрики precision и recall не зависят от сбалансированности классов и в совокупности дают довольно исчерпывающее представление о классификаторе. Однако на практике часто бывает так, что увеличение одной из метрик может привести к уменьшению другой.\n",
    "\n",
    "Концентрация только на одной метрике (precision или recall) без учёта второй — сомнительная идея.\n",
    "\n",
    "В битве за максимум precision для класса 1 побеждает модель, которая всегда будет говорить «нет». У неё вообще не будет ложноположительных срабатываний.\n",
    "\n",
    "В битве за максимум recall для класса 1 побеждает модель, которая всегда будет говорить «да». Она охватит все наблюдения класса 1, и у неё не будет ложноотрицательных срабатываний. \n",
    "\n",
    "В реальности необходимо балансировать между двумя этими метриками."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. $F_{\\beta}$ (F-мера) — это взвешенное среднее гармоническое между precision и recall:\n",
    "\n",
    "$$\n",
    "F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{precision \\cdot recall}{(\\beta^2 \\cdot precision) + recall}\n",
    "$$\n",
    "\n",
    "где $\\beta$ — это вес precision в метрике: чем больше $\\beta$, тем больше вклад.\n",
    "\n",
    "---\n",
    "\n",
    "В частном случае, когда $\\beta = 1$, мы получаем равный вклад для precision и recall, а формула будет выражать простое среднее гармоническое, или метрику $F_1$ ($F_1$-мера):\n",
    "\n",
    "$$\n",
    "F_1 = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "Рассчитаем метрику $F_1$ для нашего примера:\n",
    "\n",
    "$$\n",
    "F_1 = 2 \\cdot \\frac{0.67 \\cdot 0.57}{0.67 + 0.57} = 0.62\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### В чём преимущество $F_1$-меры?\n",
    "\n",
    "- Метрика равна своему максимуму (1), если и precision, и recall равны 1 (то есть когда отсутствуют как ложноположительные, так и ложноотрицательные срабатывания). Но если хотя бы одна из метрик близка к 0, то и $F_1$ будет близка к 0.\n",
    "\n",
    "- Несмотря на отсутствие бизнес-интерпретации, метрика $F_1$ является довольно распространённой и используется в задачах, где необходимо выбрать модель, которая балансирует между precision и recall.\n",
    "\n",
    "- Например, если цена дополнительной диагностики заболевания очень высокая, то есть ложных срабатываний должно быть минимум, но при этом важно охватить как можно больше больных пациентов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примечание: Ещё одно небольшое, но очень важное замечание: все суждения, которые мы привели по отношению к precision, recall и F-мере, относятся только к классу 1, так как эти метрики по умолчанию считаются для класса 1. Для решения большинства задач знания о значении этих метрик для класса 1 более чем достаточно, так как обычно нас интересует именно наличие некоторого эффекта.\n",
    "\n",
    "Однако если вам по каким-то причинам необходимо рассчитать precision, recall и F-меру для класса 0, для этого достаточно сделать перекодировку классов — поменять их обозначения местами или (при расчёте метрик с помощью библиотеки sklearn) изменить значение специального параметра pos_label на 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **НАЗВАНИЕ**            | **ФОРМУЛА**                                     | **ИНТЕРПРЕТАЦИЯ И ПРИМЕНЕНИЕ**                                                                                         | **ДОСТОИНСТВА**                                                                                           | **НЕДОСТАТКИ**                                                                                                            | **ФУНКЦИЯ В МОДУЛЕ METRICS БИБЛИОТЕКИ SKLEARN** |\n",
    "|-------------------------|-------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|----------------------------------------------|\n",
    "| **ACCURACY (достоверность)** | $\\frac{TP + TN}{TP + TN + FN + FP}$           | Доля правильных ответов среди всех ответов модели. Применяется в задачах, где классы сбалансированы.                  | Очень легко интерпретировать. Автоматически можно посчитать процент ошибок модели как $1 - accuracy$.       | Плохо показывает себя на сильно несбалансированных классах.                                                                | `accuracy_score()`                           |\n",
    "| **PRECISION (точность)** | $\\frac{TP}{TP + FP}$                           | Способность модели отделять класс 1 от класса 0. Используется, когда важно минимальное количество ложноположительных срабатываний. | Можно использовать на несбалансированных выборках.                                                         | Вычисляется только для положительного класса — класса 1. Для класса 0 показатель необходимо вычислять отдельно.            | `precision_score()`                          |\n",
    "| **RECALL (полнота)**     | $\\frac{TP}{TP + FN}$                           | Способность модели находить класс 1. Используется, когда важно охватить как можно больше объектов положительного класса и уменьшить количество ложноотрицательных срабатываний. | Можно использовать на несбалансированных выборках.                                                         | Вычисляется только для положительного класса. Необходимо учитывать алгоритм для каждого класса отдельно.                   | `recall_score()`                             |\n",
    "| **$F_1$-мера**           | $\\frac{2 \\cdot precision \\cdot recall}{precision + recall}$ | Нет бизнес-интерпретации. Используется в задачах, где необходимо балансировать между precision и recall. | Дает обобщённое представление о точности и полноте. Максимум достигается, когда обе метрики максимальны. Минимум — если одна из метрик равна 0. | Отсутствие интерпретации не даёт интуитивного понимания человеку, незнакомому с этой метрикой.                            | `f1_score()`                                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# «ОДИН ПРОТИВ ВСЕХ» (ONE VS OVER)\n",
    "\n",
    "Метод «один против всех» (one-vs-rest, OvR) используется для многоклассовой классификации с бинарными классификаторами. В этом подходе для каждого класса создаётся отдельная модель, которая отличает этот класс от всех остальных.\n",
    "\n",
    "Например, для задачи с тремя классами (A, B и C) OvR создаёт три модели:\n",
    "\n",
    "* Классификатор A против (B и C),\n",
    "* Классификатор B против (A и C),\n",
    "* Классификатор C против (A и B).\n",
    "\n",
    "При предсказании каждый классификатор выдает вероятность принадлежности к своему классу, и объект относят к тому классу, для которого вероятность оказалась максимальной.\n",
    "\n",
    "<img src='Images/ml_10.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Деревья решений\n",
    "\n",
    "Дерево решений — это популярный алгоритм машинного обучения, используемый для решения задач классификации и регрессии. Оно представляет собой иерархическую структуру, где данные последовательно разделяются на основании определенных условий.\n",
    "\n",
    "1. Основные элементы дерева решений:\n",
    "- Корневой узел (Root Node):\n",
    "\n",
    "2. Первый узел дерева.\n",
    "- Содержит весь набор данных.\n",
    "- Делится на основе некоторого признака, чтобы минимизировать неопределенность (например, энтропию или дисперсию).\n",
    "\n",
    "3. Внутренние узлы (Internal Nodes):\n",
    "\n",
    "- Узлы, на которых выполняются проверки на признаки.\n",
    "- Каждый узел представляет собой условие (например, Age > 30).\n",
    "\n",
    "4. Листовые узлы (Leaf Nodes):\n",
    "\n",
    "- Узлы, которые не делятся дальше.\n",
    "- Содержат итоговое предсказание или значение (класс или числовую величину).\n",
    "\n",
    "5. Ветви (Branches):\n",
    "\n",
    "- Пути, которые связывают узлы дерева.\n",
    "- Каждая ветвь соответствует определенному исходу условия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы приходите в страховую компанию, где вам дают заполнить анкету. По этой анкете сотрудник страховой компании будет принимать решение, стоит ли выдавать вам страховку.\n",
    "\n",
    "Сотрудник в свою очередь будет руководствоваться примерно следующим регламентом:\n",
    "\n",
    "<img src='Images/ml_11.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичным образом работает и алгоритм машинного обучения под названием «дерево решений» (Decision Tree).\n",
    "\n",
    "Если дерево уже обучено, то есть уже сформулированы условия в прямоугольниках, то, когда в страховую компанию придёт новый автовладелец, сотруднику будет достаточно прогнать данные клиента через дерево решений и таким образом принять решение, то есть произвести классификацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Успешнее всего деревья применяют в следующих областях:\n",
    "\n",
    "- Банковское дело. Оценка кредитоспособности клиентов банка при выдаче кредитов.\n",
    "\n",
    "- Промышленность. Контроль качества продукции (обнаружение дефектов в готовых товарах), испытания без нарушений (например, проверка качества сварки) и т. п.\n",
    "\n",
    "- Медицина. Диагностика заболеваний разной сложности.\n",
    "\n",
    "- Молекулярная биология. Анализ строения аминокислот.\n",
    "\n",
    "- Торговля. Классификация клиентов и товара."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формально структура дерева решений — это связный ациклический граф. Что это значит?\n",
    "\n",
    "* Граф — это абстрактная топологическая модель, которая состоит из вершин и соединяющих их рёбер.\n",
    "\n",
    "* Связный граф — это граф, в котором между любой парой существует направленная связь.\n",
    "\n",
    "* Ациклический граф — это граф, в котором отсутствуют циклы, то есть в графе не существует такого пути, по которому можно вернуться в начальную вершину.\n",
    "\n",
    "<img src='Images/ml_12.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Корневая вершина (root node) — то, откуда всё начинается. Это первый и самый главный вопрос, который дерево задаёт объекту. В примере со страхованием это был вопрос «Возраст автовладельца > 40».\n",
    "\n",
    "2. Внутренние вершины (intermediate nodes) — это дополнительные уточняющие вопросы, которые дерево задаёт объекту. \n",
    "\n",
    "3. Листья (leafs) — конечные вершины дерева. Это вершины, в которых содержится конечный «ответ» — класс объекта.\n",
    "\n",
    "Максимально возможная длина от корня до самых дальних листьев (не включая корневую) называется максимальной **глубиной дерева (max depth)**.\n",
    "\n",
    "Во внутренней или корневой вершине признак проверяется на некий логический критерий, по результатам которого мы движемся всё глубже по дереву. Например, «Количество кредитов <= 1».\n",
    "\n",
    "Логический критерий, который находится в каждой вершине, называется **предикатом**, или решающим правилом."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
