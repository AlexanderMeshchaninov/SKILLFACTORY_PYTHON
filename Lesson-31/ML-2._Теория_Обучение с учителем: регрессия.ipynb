{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# РЕГРЕССИЯ\n",
    "\n",
    "<img src='Images/ml_10.png' width='900' height='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В категории обучения с учителем модели можно условно разделить на следующие основные типы:\n",
    "\n",
    "* Линейные модели: линейная регрессия (для задачи регрессии) и логистическая регрессия (для задачи классификации) и производные от них.\n",
    "* «Древесные» модели: дерево решений и производные от него. \n",
    "* Метрические алгоритмы: метод ближайших соседей и производные от него.\n",
    "* Байесовские методы: метод наивного Байеса и производные от него.\n",
    "* Ансамблевые методы: композиции из методов (бэггинг, стекинг, бустинг)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Линейные модели — это модели, отображающие зависимость целевого признака от факторов в виде линейной взаимосвязи.\n",
    "\n",
    "<img src='Images/ml_11.png' width='900' height='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данном графике мы видим зависимость цены товара от его размера. Из диаграммы рассеяния видно, что в среднем точки расположены на одной прямой линии. То есть зависимость линейная.\n",
    "\n",
    "Подкласс линейных моделей в свою очередь содержит множество конкретных моделей. В библиотеке sklearn, которую будет использоваться, все линейные алгоритмы содержатся в модуле [linear_model](https://scikit-learn.ru/1-1-linear-models/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ОБЩЕЕ ПРЕДСТАВЛЕНИЕ О ЛИНЕЙНОЙ РЕГРЕССИИ\n",
    "\n",
    "Линейная регрессия (Linear Regression) — одна из простейших моделей для решения задачи регрессии. Главная гипотеза состоит в том, что рассматриваемая зависимость является линейной.\n",
    "\n",
    "Общий вид модели в случае, когда целевая переменная зависит от  факторов, будет иметь следующий вид:\n",
    "\n",
    "$\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_m x_m$\n",
    "\n",
    "Давайте разбираться, что в этом выражении значит каждая из переменных. Начнём с простого — с двумерного случая.\n",
    "\n",
    "## 2D-случай\n",
    "\n",
    "Для начала поговорим о самом простом случае, когда у нас есть один фактор и зависящий от него целевой признак. Геометрически такая зависимость представляет собой координатную плоскость, где мы отмечаем точки по оси x и соответствующие им точки на оси y.\n",
    "\n",
    "Рассмотрим задачу из нефтяной отрасли. Есть набор данных, где представлены данные о средней пористости скважин (в процентах) и добыче газа на этих скважинах в сутки (в миллионах кубических футов). \n",
    "\n",
    "Нам бы хотелось построить модель, которая опишет зависимость и позволит по известной пористости скважин предсказывать неизвестную выработку газа.\n",
    "\n",
    "Зависимость целевого признака от фактора представлена на диаграмме рассеяния (см. ниже). Пористость скважины отложена по оси абсцисс — Porosity (%), а добыча газа — по оси ординат, Gas production (Mcf/day).\n",
    "\n",
    "<img src='Images/ml_12.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из диаграммы отчётливо видно, что с ростом пористости скважины растёт добыча газа. Причём растёт она преимущественно линейно: основная масса точек находится на одной прямой.\n",
    "\n",
    "Идея! Давайте проведём через точки прямую линию так, чтобы она максимально хорошо описывала зависимость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для этого сначала вспомним уравнение прямой из школьного курса математики:\n",
    "\n",
    "$y = kx + b$\n",
    "\n",
    "где:\n",
    "\n",
    "* $x$ — это некоторый фактор, от которого зависит целевая переменная $y$. В нашем случае, $x$ — это пористость скважины, а $y$ — добыча газа.\n",
    "* $k$ — коэффициент наклона прямой (тангенс угла наклона). Если $k > 0$, это означает, что угол наклона прямой острый и прямая возрастает. Если $k < 0$, угол наклона тупой и прямая убывает.\n",
    "$b$ — коэффициент смещения прямой по оси $y$. Он будет соответствовать значению $y$ при $x$. То есть это точка пересечения прямой и оси Y.\n",
    "\n",
    "<img src='Images/ml_13.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На данном графике изображены две прямые с разными коэффициентами наклона. Зелёная прямая соответствует положительному значению $k_1 > 0$, и геометрически $k_1$ равен тангенсу острого угла $\\alpha_1$ наклона прямой по отношению к оси $x$: $k_1 = tg(\\alpha_1)$. Синяя прямая соответствует отрицательному значению $k_2 < 0$, и геометрически $k_2$ равен тангенсу тупого угла $\\alpha_2$ наклона прямой по отношению к оси $x$: $k_2 = tg(\\alpha_2)$. Каждая из прямых пересекается с осью $y$ в точках $b_1$ и $b_2$ — это и есть **коэффициент смещения прямых**.\n",
    "\n",
    "Это уравнение и есть двумерная модель линейной регрессии. Зная коэффициенты $k$ и $b$, мы можем подставить их в любое пористость скважины $x$ и получить предсказание добычи газа $y$.\n",
    "\n",
    "Однако в машинном обучении приняты немного другие обозначения. Фактическое значение целевой переменной обозначается как $y$, а вот предсказанное моделью — $\\hat{y}$. Также для удобства коэффициенты $b$ и $k$ приведём к единому обозначению: $w_0 = b$ и $w_1 = k$. Тогда уравнение модели линейной регрессии запишется в виде:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x$$\n",
    "\n",
    "**Примечание**. Коэффициенты $w_0$ и $w_1$ называются **параметрами линейной регрессии**.\n",
    "\n",
    "Остаётся только один вопрос: откуда, собственно, взять параметры $w_0$ и $w_1$? Обсудим этот вопрос чуть позже.\n",
    "\n",
    "А пока представим, что параметры мы нашли. В таком случае можно построить прямую, которая опишет нашу зависимость. Пусть коэффициенты составляют (мы их нашли сами по методу наименьших квадратов, о котором поговорим ниже):\n",
    "\n",
    "$$w_0 = -2.94$$  \n",
    "$$w_1 = 287.7$$\n",
    "\n",
    "Тогда модель будет иметь следующий вид:\n",
    "\n",
    "$$\\hat{y} = 287.7x - 2.94$$\n",
    "\n",
    "Если подставить значения конкретные значения пористости $x$ в модель, можно построить прямую, которая описывает исходную зависимость. Это и будет графическая интерпретация нашей модели:\n",
    "\n",
    "<img src='Images/ml_14.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общий случай\n",
    "\n",
    "А что если факторов не два, а больше: 3, 15, 100? Тут-то мы и приходим к общему виду модели линейной регрессии, который вводили в самом начале. Пусть у нас есть $m$ факторов $\\{x_1, x_2, \\dots, x_m\\}$, от которых зависит целевая переменная $y$.\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_m x_m = w_0 + \\sum_{j=1}^{m} w_j x_j$$\n",
    "\n",
    "В геометрическом смысле данное уравнение описывает плоскость в $(m + 1)$-мерном пространстве ($m$ факторов + 1 целевой признак отложены по осям координат). Такую плоскость называют **гиперплоскостью**.\n",
    "\n",
    "Абстрактное $(m + 1)$-мерное пространство, конечно же, невозможно отобразить графически и сложно даже представить, как оно выглядит. Но нам это и не нужно. Все операции в таком пространстве аналогичны операциям в двумерном или трёхмерном пространстве.\n",
    "\n",
    "> Для понимания принципа работы мы будем рассматривать только прямую в двумерном пространстве, а результат уже обобщать на случай с большей размерностью.\n",
    "\n",
    "Стоит отметить, что в DS мы, как правило, работаем с большим количеством факторов (больше двух), которые описывают данные, поэтому отобразить модель в геометрическом пространстве не получится, но важно понимать, что представляет собой сама модель."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ПОИСК ПАРАМЕТРОВ ЛИНЕЙНОЙ РЕГРЕССИИ: МЕТОД НАИМЕНЬШИХ КВАДРАТОВ\n",
    "\n",
    "Теперь мы знаем, как выглядит модель линейной регрессии в общем случае: это простое линейное выражение, подставляя в которое значения факторов, можно найти целевую переменную. Это линейное выражение соответствует прямой, плоскости или гиперплоскости в зависимости от количества признаков.\n",
    "\n",
    "Метод наименьших квадратов — это основной способ поиска параметров (коэффициентов) линейной регрессии. Его цель — минимизировать разницу между предсказанными моделью значениями и фактическими значениями целевой переменной.\n",
    "\n",
    "Что вообще есть ошибка? В самом простом понимании это расхождение между истиной и предсказанием.\n",
    "\n",
    "Осталось только понять: где взять эту функцию ошибки? Ответ кроется в картинке ниже. Давайте представим, как могла бы выглядеть прямая в двумерном пространстве, проведённая, например, через пять точек:\n",
    "\n",
    "<img src='Images/ml_15.png' width='600' height='500'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы не учитывать знак расхождения, можно взять модуль разницы между истинным значением и предсказанным (тем, что лежит на прямой). Рассчитать ошибки $e_i$ (на рисунке они отмечены красными отрезками) для всех пяти точек можно следующим образом:\n",
    "\n",
    "$$e_i = |y_i - \\hat{y_i}|$$\n",
    "\n",
    "где $y_i$ — это результат подстановки $i$-ого значения $x$ в модель линейной регрессии.\n",
    "\n",
    "Вычислим среднее по всем ошибкам. Такая ошибка называется **средняя абсолютная ошибка (Mean Absolute Error, MAE)** и записывается следующим образом (в двумерном случае):\n",
    "\n",
    "$$MAE = \\frac{\\sum_{i=1}^{n} e_i}{n} = \\frac{\\sum_{i=1}^{n} |y_i - \\hat{y_i}|}{n} = \\frac{\\sum_{i=1}^{n} |y_i - w_0 - w_1 x_i|}{n}$$\n",
    "\n",
    "Осталось только найти такие $w_0$ и $w_1$, при которых $MAE$ была бы минимальной. В математике это записывается следующим образом:\n",
    "\n",
    "$$MAE = \\frac{\\sum_{i=1}^{n} e_i}{n} = \\frac{\\sum_{i=1}^{n} |y_i - \\hat{y_i}|}{n} = \\frac{\\sum_{i=1}^{n} |y_i - w_0 - w_1 x_i|}{n} \\rightarrow \\min_{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку MAE используется довольно редко, можно вместо модуля использовать квадрат — он тоже убирает знак ошибки и по сути аналогичен модулю. Получим **среднеквадратичную ошибку (Mean Square Error, MSE)**:\n",
    "\n",
    "$$MSE = \\frac{\\sum_{i=1}^{n} e_i^2}{n} = \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{n} = \\frac{\\sum_{i=1}^{n} (y_i - w_0 - w_1 x_i)^2}{n}$$\n",
    "\n",
    "Это и будет наша функция ошибки, которую мы будем минимизировать, управляя параметрами $w_0$ и $w_1$:\n",
    "\n",
    "$$MSE = \\frac{\\sum_{i=1}^{n} e_i^2}{n} = \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{n} = \\frac{\\sum_{i=1}^{n} (y_i - w_0 - w_1 x_i)^2}{n} \\rightarrow \\min_{w}$$\n",
    "\n",
    "Примечание: В общем случае, когда $X$ — это таблица из $n$ наблюдений и $m$ признаков, постановка задачи оптимизации MSE выглядит следующим образом:\n",
    "\n",
    "$$MSE = \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y_i})^2}{n} = \\frac{\\sum_{i=1}^{n} \\left( y_i - w_0 - \\sum_{j=1}^{m} w_j x_{ij} \\right)^2}{n} \\rightarrow \\min_{w},$$\n",
    "\n",
    "где $x_{ij}$ — это значение, которое находится в $i$-ой строке и $j$-ом столбце таблицы наблюдений.\n",
    "\n",
    "Математике известно решение данной задачи оптимизации. Метод поиска параметров линейной регрессии называется **методом наименьших квадратов** (сокращённо — **МНК**) и был изобретён Гауссом ещё в 1795 году. В английской литературе часто можно встретить аббревиатуру **OLS (Ordinary Least Squares)**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Images/ml_16.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# МЕТРИКИ РЕГРЕССИИ\n",
    "\n",
    "Начнем с того, что нам необходимо научиться оценивать качество модели с помощью какого-то показателя (или нескольких показателей). Такой показатель в машинном обучении называется *метрикой*. И для каждого класса задач машинного обучения существуют свои метрики.\n",
    "\n",
    "Метрика — это численное выражение качества моделирования.\n",
    "\n",
    "Для оценки качества решения задачи регрессии существует множество метрик. Давайте рассмотрим часто используемые.\n",
    "\n",
    "Будем рассматривать метрики для задачи регрессии на следующем примере. Возьмём первые пять наблюдений из нашей таблицы и предсказанные для них моделью lr_lstat ответы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этих значениях мы будем рассматривать следующие метрики:\n",
    "\n",
    "### 1. Средняя абсолютная ошибка — MAE (Mean Absolute Error)\n",
    "\n",
    "Это самый простой и уже знакомый вам показатель. Чтобы посчитать данную метрику, нужно найти все остатки (разницы между предсказанным значением и реальным), взять от каждого из них модуль, сложить их и поделить на количество. Иными словами, нам нужно найти среднее арифметическое модуля отклонения предсказанного значения от реального.\n",
    "\n",
    "$ MAE = \\frac{ \\sum_{i=1}^{n} |y_i - \\hat{y_i}| }{n} $\n",
    "\n",
    "Данная метрика интерпретируется очень легко: это число показывает, насколько в среднем наша модель ошибается. Чем меньше значение метрики, тем лучше.\n",
    "\n",
    "$ MAE = \\frac{|24.0 - 29.82| + |21.6 - 25.87| + |34.7 - 30.73| + |33.4 - 31.76| + |36.2 - 29.49|}{5} = 4.482 \\, \\text{[тыс. \\$]} $\n",
    "\n",
    "То есть для нашего примера из пяти наблюдений в среднем модель ошибается на 4.482 тысячи долларов.\n",
    "\n",
    "Много ли это? Хороший вопрос, на который без эксперта-оценщика недвижимости будет сложно дать ответ. Однако можно попробовать посчитать ошибку в процентах, ведь в процентах всё воспринимается легче, и для этого нам пригодится следующая метрика — **MAPE**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Средняя абсолютная ошибка в процентах — MAPE (Mean Absolute Percent Error)\n",
    "\n",
    "Для её вычисления мы делим модуль разницы между предсказанием алгоритма и истинным значением на истинное значение. Затем складываем все результаты (для каждого объекта), делим на количество и умножаем на 100%.\n",
    "\n",
    "$ MAPE = \\frac{ \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y_i}}{y_i} \\right| }{n} \\cdot 100\\% $\n",
    "\n",
    "Эта метрика показывает, на сколько процентов в среднем наше предсказание отклоняется от реального значения. Эта метрика отлично показывает себя в задачах, когда неизвестно, какое значение целевого показателя считать приемлемым.\n",
    "\n",
    "Например, средняя ошибка — 2 тысячи долларов. Это много или мало? Смотря для чего... А вот средняя ошибка, равная 80 % — это много или мало? Определённо много.\n",
    "\n",
    "$ MAPE = \\frac{|24.0 - 29.82|}{24.0} + \\frac{|21.6 - 25.87|}{21.6} + \\frac{|34.7 - 30.73|}{34.7} + \\frac{|33.4 - 31.76|}{33.4} + \\frac{|36.2 - 29.49|}{36.2} \\cdot \\frac{100\\%}{5} = 15.781\\% $\n",
    "\n",
    "Таким образом, на первых пяти наблюдениях модель в среднем ошибается на 15.781%. Это довольно неплохой результат.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Средняя квадратическая ошибка — MSE\n",
    "\n",
    "Данный показатель мы используем в линейной регрессии в качестве функции потерь, но ничто не мешает нам также использовать его и в качестве метрики.\n",
    "\n",
    "Логика вычисления данной ошибки очень похожа на предыдущую. Разница лишь в том, что вместо модуля разности между предсказанным и реальным значениями мы берём квадрат этого модуля:\n",
    "\n",
    "$ MSE = \\frac{ \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 }{n} $\n",
    "\n",
    "Данная метрика хуже поддаётся интерпретации, чем предыдущая, так как измеряется не в единицах, а в квадратах единиц. Она чаще используется для внутреннего обсуждения между дата-сайентистами, заказчику такая метрика может быть непонятна.\n",
    "\n",
    "$ MSE = \\frac{(24.0 - 29.82)^2 + (21.6 - 25.87)^2 + (34.7 - 30.73)^2 + (33.4 - 31.76)^2 + (36.2 - 29.49)^2}{5} = 22.116 \\, [ \\text{тыс. \\$}^2 ] $\n",
    "\n",
    "Таким образом, для нашего примера квадрат отклонения составляет 22.116 тысяч долларов в квадрате.\n",
    "\n",
    "Согласитесь, не очень понятно, о чём идёт речь. Однако данная метрика является популярной, так как позволяет \"штрафовать\" модель за более большие ошибки.\n",
    "\n",
    "### Что значит «штрафовать»? \n",
    "\n",
    "Например, расхождение в 200 единиц в метрике MSE воспринимается как $200^2$, а в метрике MAE это расхождение воспринимается как 200. Поэтому, если у нас есть две модели, но одна из них допускает большие ошибки, эти ошибки становятся ещё больше при расчёте метрики MSE, и нам легче сравнить модели между собой.\n",
    "\n",
    "Но в то же время это и проклятие MSE. Если в данных присутствуют выбросы, метрика может быть необъективной. Если модель будет утверждать, что цена здания — 30 тысяч долларов, а в наборе данных ему соответствует цена в 3 миллиона долларов, то при возведении такой ошибки в квадрат получится 9 миллионов, что может сбить с толку исследователя. Необходимо скептически относиться к данной метрике, если вы не проводили исследование данных на предмет наличия выбросов.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Корень из средней квадратической ошибки — RMSE (Root Mean Squared Error)\n",
    "\n",
    "Для получения RMSE надо просто извлечь квадратный корень из MSE:\n",
    "\n",
    "$ RMSE = \\sqrt{MSE} = \\sqrt{ \\frac{ \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 }{n} } $\n",
    "\n",
    "Корень извлекается для того, чтобы привести размерности ответов и ошибок в соответствие и сделать метрику более понятной.\n",
    "\n",
    "$ RMSE = \\sqrt{22.116} = 4.702 \\, [\\text{тыс. \\$}] $\n",
    "\n",
    "Преимущества и недостатки этой метрики такие же, как и у MSE, к преимуществам добавляется только понятная размерность.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Коэффициент детерминации ($R^2$)\n",
    "\n",
    "Все рассматриваемые ранее метрики имели масштаб от $0$ до $+\\infty$. Чем это плохо?\n",
    "\n",
    "А что если нам скажут, что $MSE$ для модели составляет 32? Должны ли мы улучшить модель, или она достаточно хороша? А что если $MSE = 0.4$?\n",
    "\n",
    "На самом деле, трудно понять, хороша модель или нет, не сравнив её показатели с теми же показателями других моделей.\n",
    "\n",
    "Коэффициент детерминации, или $R^2$, является ещё одним показателем, который мы можем использовать для оценки модели. Он тесно связан с $MSE$, но его преимущество в том, что $R^2$ всегда находится в промежутке между $-\\infty$ и 1.\n",
    "\n",
    "$ R^2 = 1 - \\frac{MSE}{MSE_{mean}}, $\n",
    "\n",
    "где\n",
    "\n",
    "$ MSE_{mean} = \\frac{ \\sum_{i=1}^{n} (y_i - \\bar{y})^2 }{n}. $\n",
    "\n",
    "где $\\bar{y}$ — среднее по вектору правильных ответов.\n",
    "\n",
    "То есть $R^2$ показывает, насколько наша модель лучше, чем если бы все предсказания были средним по правильным ответам.\n",
    "\n",
    "Посмотрим, как считается $R^2$. Сначала рассчитаем среднее по правильным ответам:\n",
    "\n",
    "$$ \\bar{y} = \\frac{24.0 + 21.6 + 34.7 + 33.4 + 36.2}{5} = 29.98 $$\n",
    "\n",
    "Теперь рассчитаем $MSE_{mean}$:\n",
    "\n",
    "$$ MSE_{mean} = \\frac{(24.0 - 29.98)^2 + (21.6 - 29.98)^2 + (34.7 - 29.98)^2 + (33.4 - 29.98)^2 + (36.2 - 29.98)^2}{5} = 35.72 $$\n",
    "\n",
    "И, наконец, сам $R^2$:\n",
    "\n",
    "$$ R^2 = 1 - \\frac{22.116}{35.72} = 0.38 $$\n",
    "\n",
    "Есть ещё одна интерпретация данной метрики. Статистически показатель $R^2$ описывает, какую долю информации о зависимости (дисперсии) смогла уловить модель.\n",
    "\n",
    "> Удовлетворительным $R^2$ считается показатель выше 0.5: чем ближе к 1, тем лучше. Отрицательные значения $R^2$ говорят о том, что построенная модель настолько плоха, что лучше было бы присвоить всем ответам среднее значение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Название** | **Формула**                               | **Интерпретация и применение**                                                                                       | **Достоинства**                                                                                  | **Недостатки**                                                                                                                                          | **Функция в модуле metrics библиотеки sklearn**        |\n",
    "|--------------|--------------------------------------------|----------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------|\n",
    "| **MAE**      | $MAE = \\frac{\\sum_{i=1}^{n} \\lvert y_i - \\hat{y_i} \\rvert}{n}$ | Помогает оценить абсолютную ошибку: насколько в среднем число в предсказании разошлось с реальным числом.             | Удобно интерпретировать. Измеряется в тех же единицах, что и целевой признак. Не сильно искажается при наличии выбросов. | Не поможет, если необходимо сравнивать модели, предсказывающие одно и то же по разным признакам. | `mean_absolute_error()` |\n",
    "| **MAPE**     | $MAPE = \\frac{100 \\%}{n} \\sum_{i=1}^{n} \\left\\lvert \\frac{y_i - \\hat{y_i}}{y_i} \\right\\rvert$ | Помогает абстрагироваться от конкретных чисел и оценить абсолютную ошибку в процентах. | Легко интерпретировать. Используется в задачах, где неизвестно, какое значение метрики считать приемлемым. | Плохо подходит для задач, в которых важны конкретные единицы измерений. Лучше использовать в паре с MAE, чтобы знать абсолютную ошибку и её значение в процентах. | `mean_absolute_percentage_error()` |\n",
    "| **MSE**      | $MSE = \\frac{ \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 }{ n }$ | Интерпретации нет. Используется в задачах, где критически важны большие ошибки, например при предсказании координат полёта. | Каждая ошибка вносит свой квадратичный штраф, большие расхождения между предсказанием и истиной увеличивают штраф. | Измеряется в квадратах единиц, поэтому менее доступна для понимания. Искажается при наличии выбросов. Не поможет, если нужно сравнить модели, предсказывающие одно и то же по разным признакам. | `mean_squared_error()` |\n",
    "| **RMSE**     | $RMSE = \\sqrt{ \\frac{ \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 }{ n } }$ | Можно трактовать как стандартное отклонение предсказаний от истинных ответов. Используется в тех же задачах, что и MSE. | Имеет те же преимущества, что и MSE, но более удобна для понимания (измеряется в тех же единицах, что и целевая переменная). | Не поможет, если нужно сравнить модели, предсказывающие одно и то же по разным признакам. | Отдельная функция отсутствует, но можно извлечь корень из результата функции `mean_squared_error()` |\n",
    "| **$R^2$**    | $R^2 = 1 - MSE / MSE_{mean}$         | Помогает понять, какую долю разнообразия (дисперсии) смогла уловить модель в данных. Позволяет сравнить, насколько модель лучше, чем простое предсказание средним. | Можно сравнивать модели, обученные на разных признаках. Легко оценить качество модели: измеряется от $-\\infty$ до 1. Удовлетворительным показателем считается показатель выше 0.5. | Чувствительна к добавлению новых данных. Чувствительна к выбросам, так как основана на MSE. | `r2_score()` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# АЛГОРИТМ ГРАДИЕНТНОГО СПУСКА\n",
    "\n",
    "Градиентный спуск (Gradient descent) — это оптимизационный алгоритм, используемый для нахождения минимального значения функции (например, функции потерь в машинном обучении).\n",
    "\n",
    "Основная идея состоит в том, чтобы алгоритм постепенно обновлял параметры модели (например, веса $w$) в направлении наибольшего уменьшения функции потерь. Это направление определяется градиентом — вектором производных функции потерь по каждому параметру.\n",
    "\n",
    "Градиентный спуск — самый используемый алгоритм минимизации функции потерь. Он применяется почти в каждой модели машинного обучения и является наиболее простым в реализации из всех методов численной оптимизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Иллюстрация работы алгоритма градиентного спуска:\n",
    "\n",
    "<img src='Images/ml_17.png' height='700' width='1000'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описанный нами алгоритм можно перевести на язык математики. Он то и будет называться *алгоритмом градиентного спуска*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cуществует внешний параметр алгоритма (гиперпараметр) для настройки алгоритма градиентного спуска, называемый темп обучения.\n",
    "\n",
    "Темп обучения — это гиперпараметр, который определяет размер шага, на который обновляются параметры модели на каждой итерации в направлении антиградиента (направления наибольшего уменьшения функции потерь). Темп обучения обозначается как $a$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подведём промежуточный итог\n",
    "\n",
    "Градиентный спуск — простой и мощный алгоритм оптимизации, который позволяет итеративно находить минимум функции потерь и тем самым находить оптимальные параметры модели.\n",
    "\n",
    "Причём функция потерь не обязательно должна быть MSE. Главное требование к функции потерь — это её гладкость во всех точках.\n",
    "\n",
    "Благодаря своей простоте алгоритм обладает минимальной вычислительной сложностью и работает быстрее, чем метод наименьших квадратов, даже на огромных наборах данных с тысячами признаков.\n",
    "\n",
    "### Стохастический градиентный спуск (SGD)\n",
    "\n",
    "Стохастический градиентный спуск — это разновидность градиентного спуска, где обновление параметров выполняется на основе одного случайного примера из набора данных на каждой итерации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# СМЕЩЕНИЕ И РАЗБРОС\n",
    "\n",
    "Смещение и разброс — это два ключевых понятия в машинном обучении, которые помогают понять ошибки модели и её способность к обобщению.\n",
    "\n",
    "1. Смещение (Bias):\n",
    "Смещение показывает, насколько сильно модель упрощает задачу, пренебрегая особенностями данных.\n",
    "Высокое смещение: Модель слишком проста и не захватывает зависимости в данных (недообучение).\n",
    "\n",
    "Пример: Линейная модель пытается аппроксимировать данные с нелинейной зависимостью.\n",
    "\n",
    "**Недообучение (underfitting)** — проблема, обратная переобучению. Модель из-за своей слабости не уловила никаких закономерностей в данных. В этом случае ошибка будет высокой как для тренировочных данных, так и для данных, не показанных во время обучения.\n",
    "\n",
    "2. Разброс (Variance):\n",
    "Разброс характеризует, насколько сильно предсказания модели изменяются при использовании разных обучающих выборок.\n",
    "Высокий разброс: Модель слишком чувствительна к небольшим изменениям в данных (переобучение).\n",
    "\n",
    "Пример: Сложная модель точно подстраивается под обучающие данные, но плохо работает на новых примерах.\n",
    "\n",
    "**Переобучением (overfitting)** - когда такая модель работает намного лучше с обучающими данными, чем с новыми. Она была чрезмерно натренирована на обнаружение уникальных характеристик обучающего набора данных, которые не являются общими закономерностями.\n",
    "\n",
    "<center><img src='Images/ml_18.png' height='300' width='800'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* На первом рисунке изображена простая модель линейной регрессии, не способная уловить сложную зависимость в данных.\n",
    "\n",
    "* На втором рисунке изображена оптимальная модель, которая хорошо описывает зависимость и при этом не имеет переобучения (полином четвёртой степени).\n",
    "\n",
    "* На последнем рисунке изображен полином 27-й степени, который подстроился под каждую точку в тренировочном наборе, но не смог уловить общие закономерности."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дилемма смещения-дисперсии \n",
    "\n",
    "Является центральной проблемой в обучении с учителем. В идеале мы хотим построить модель, которая точно описывает зависимости в тренировочных данных и хорошо работает на неизвестных данных. К сожалению, обычно это невозможно сделать одновременно.\n",
    "\n",
    "- Усложняя модель, мы пытаемся уменьшить смещение (bias), однако появляется риск получить переобучение, то есть мы повышаем разброс (variance). \n",
    "\n",
    "- Снизив разброс (variance) позволяет более простой модели, не склонной к переобучению, получить риск не уловить зависимостей и окажеться недообученной, то есть мы повышаем смещение (bias).\n",
    "\n",
    "<center><img src='Images/ml_19.png' height='400' width='450'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы знаем о теоретических основах проблемы переобучения и недообучения, что мы можем сделать, чтобы лучше судить о способности модели к обобщению на практике? Как диагностировать высокие bias и variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестовый и тренировочные наборы данных\n",
    "\n",
    "Типичным решением является разделение данных на две части: обучающий и тестовый наборы. На тренировочном наборе данных мы будем обучать модель, подбирая параметры. Тестовый набор данных, который модель не видела при обучении, мы будем использовать для оценки истинного качества моделирования. Схематично можно представить это следующим образом:\n",
    "\n",
    "<center><img src='Images/ml_20.png' height='400' width='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ПОЛИНОМИАЛЬНЫЕ ПРИЗНАКИ\n",
    "\n",
    "Полиномиальная регрессия (Polynomial Regression) — это более сложная модель, чем линейная регрессия. Вместо уравнения прямой используется уравнение полинома (многочлена). Степень полинома может быть сколь угодно большой: чем больше степень, тем сложнее модель.\n",
    "\n",
    "Полиномиальная регрессия\n",
    "Полиномиальная регрессия — это обобщение линейной регрессии, где зависимость между целевой переменной и признаками выражается как полином.\n",
    "\n",
    "Модель: $\\hat{y} = w_0 + w_1 x + w_2 x^2 + \\dots + w_n x^n$\n",
    "\n",
    "* $x$ - признак, $n$ - степень полинома.\n",
    "\n",
    "* Коэффициенты $w_0$, $w_1$, $\\dots$, $w_n$ определяются методом наименьших квадратов.\n",
    "\n",
    "### Когда использовать:\n",
    "\n",
    "* Когда данные имеют нелинейную зависимость, и простая линейная регрессия не даёт хороших результатов.\n",
    "\n",
    "### Преимущества:\n",
    "* Модель улавливает сложные зависимости в данных.\n",
    "\n",
    "### Недостатки:\n",
    "* Переобучение на больших степенях.\n",
    "\n",
    "* Трудно интерпретировать результаты при высокой степени полинома.\n",
    "\n",
    "<img src='Images/ml_21.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построить полиномиальную регрессию в sklearn очень просто. Для начала необходимо создать полиномиальные признаки с помощью объекта класса PolynomialFeatures из модуля preprocessing. Это преобразователь, который позволит сгенерировать полиномиальные признаки любой степени и добавить их в таблицу. У него есть два важных параметра:\n",
    "\n",
    "* degree — степень полинома. По умолчанию используется степень 2.\n",
    "\n",
    "* include_bias — включать ли в результирующую таблицу столбец из единиц (x в степени 0). По умолчанию стоит True, но лучше выставить его в значение False, так как столбец из единиц и так добавляется в методе наименьших квадратов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примечание: Как правило, дата-сайентисты останавливаются на полиноме второй (максимум третьей) степени. Чем выше степень полинома, тем больше слагаемых, а значит, тем больше признаков и тем сложнее становится модель!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# РЕГУЛЯРИЗАЦИЯ\n",
    "\n",
    "Регуляризация — это метод предотвращения переобучения модели, добавляя штраф за сложность модели (например, за большие коэффициенты) в функцию потерь.\n",
    "\n",
    "Идея регуляризации состоит в том, что мы намеренно пытаемся увеличить смещение модели, чтобы уменьшить разброс. Закон баланса в действии!\n",
    "\n",
    "Математически это будет очень простая операция — добавление к функции потерь некоторого штрафа.\n",
    "\n",
    "Штраф — это дополнительное неотрицательное слагаемое в выражении для функции потерь, которое специально повышает ошибку. За счёт этого слагаемого метод оптимизации (OLS или SGD) будет находить не истинный минимум функции потерь, а псевдоминимум."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть несколько способов добавления штрафа к функции потерь:\n",
    "\n",
    "- **L1-регуляризация (Lasso)** — добавление к функции потерь суммы модулей коэффициентов, умноженных на коэффициент регуляризации $\\alpha$:\n",
    "\n",
    "  $L_1(w) = MSE + \\alpha \\sum_{j=1}^{m} |w_j|$\n",
    "\n",
    "- **L2-регуляризация (Ridge)**, или регуляризация Тихонова — добавление к функции потерь суммы квадратов коэффициентов, умноженных на коэффициент регуляризации $\\alpha$:\n",
    "\n",
    "  $L_2(w) = MSE + \\alpha \\sum_{j=1}^{m} (w_j)^2$\n",
    "\n",
    "> **Коэффициент $\\alpha$ (альфа)** — это коэффициент регуляризации. Он отвечает за то, насколько сильное смещение мы будем вносить в модель: чем оно больше, тем сильнее будет штраф за переобучение."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
